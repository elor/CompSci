Di 16. Okt 09:18:28 CEST 2012

# Computer-Experiment

Entwicklungsprozess:
Physikalisches System
-> Modell (idealisierte Form)
-> Algorithmus
-> Daten
-> Vergleich mit Realitaet

*Simulationen ersetzen nicht das Denken*

(Geschichten zu Programmiersprachen und FORTRAN/Java im Allgemeinen)

Java Stichw"orter
* Encapsulation
* Inheritance
* Polymorphismus/Interfaces

Hinweis:
*Erstes Programm nur Programmentwurf*

## Inhalte aus erstem Buchkapitel
* Physikalischer Hintergrund
* Modell und Algorithmus
* Probleme (aus Buch), L"osungen, Vergleich mit bekannten L"osungen/Ergebnissen
  Kontrolle von Erhaltungsgr"o"sen
  Symmetrieerhaltung
  Ist Ergebnis sinnvoll?
  statistische Fehler (Konvergenz gegeben und sinnvoll?)
* abgeschlossenes Bild
  insgesamt Kohaerent
  sch"ones Gesamtbild aus Einzelergebnissen bilden
* Varianten durchspielen

# 2. Einfache Beispiele
## 2.1. Freier Fall
### Physikalischer Hintergrund

Kugel im Gravitationsfeld ohne Reibung

F_g = -m*g
0 < g = 9.81 m/s^2

### Modell

Newtonsche Bewegungsgleichung

F=m \ddot y

analytische L"osung m"oglich:

y(t) = y(0) + v(0)*t - 1/2 gt^2
v(t) = v(0) - gt

y(0), v(0) sind Anfangsbedingungen

numerischer Ansatz:

dy/dt = v(t)
dv/dt = -g

N"aherung durch finite Differenzen:

(y(t+\Delta_t) - y(t)) / \Delta_t = v(t)
(v(t+\Delta_t) - v(t)) / \Delta_t = g

### Algorithmus
Aufl"osen nach Euler-Algorithmus-vertr"aglichen Variablen

v(t+\Delta_t) = v(t) - g \Delta_t
y(t+\Delta_t) = y(t) - v(t) \Delta_t

## 2.2. Programmierung

### Anwendungen in OSP bzw. Buch

* FirstFallingBallApp
* FallingBallApp
* FallingParticleCalcApp
* FallingParticlePlotApp

## 2.3. Harmonischer Oszillator

### Physikalischer Hintergrund

F = -ky

k - Federkonstante

### Modell

\dot y = v
\dot v = -k/m y

analytische L"osung

y(t) = A \cos{\omega_0 t} + B \sin{\omega_0 t}

A = y(0)
B = v(0)
=> erf"ullt beide Differentialgleichungen

Allerdings existieren auch andere analytische L"osungen, welche "aquivalent
und in einander "uberf"uhrbar sind. Daf"ur ist die "Uberf"uhrbarkeit der
trigonometrischen Funktionen verantwortlich.

\omega_0^2 = k/m

\omega_0 - Eigenfrequenz

### Algorithmus

Euler-Algorithmus aus 2.2, welcher allerdings nur bedingt zu empfehlen ist.
Eine kleine "Anderung hinsichtlich besserer Konvergenz ist notwendig:

Wir vertauschen die Berechnungsreihenfolge: Erst v, dann y
=> Euler-Cromer-Algorithmus

``
v(t + \Delta_t) = v(t) - k/m y(t) \Delta_t
y(t + \Delta_t) = y(t) + v(t + \Delta_t) \Delta_t
``

# 3. Allgemeine numerische L"osungen der Newtonschen Bewegungsgleichungen

\dot y = v(t)
\dot v = a(t) = F(t) / m

## 3.1. L"osungsverfahren

### Euler-Algorithmus

Formulierung: s. oben

neues y aus altem v
neues v aus altem y

### Euler-Cromer-Algorithmus

Formulierung: s. oben

neues v aus altem y
neues y aus neuem v

Erhaltung der Gesamtenergie (hier: harmonischer Oszillator)

E_{tot} = m/2 v^2 + k/2 y^2

Nur f"ur konservative (oftmals auch reversible) Systeme

### Taylor-Entwicklung in \Delta_t

``
t_n := n \Delta_t

v_{n+1} = v_n + a_n \Delta_t + \BigO{\Delta_t^2}                       (i)
x_{n+1} = x_n + v_n \Delta_t + 1/2 a_n \Delta_t^2 + \BigO{\Delta_t^3}  (ii)
``

Euler: ausschlie"slich \BigO{n}-Terme ber"ucksichtigt
=> lokaler Fehler \BigO{\Delta_t^2}
N = 1 / \Delta_t Schritte
=> globaler Fehler bis zum Ende der Simulation ist \BigO(\Delta_t)

=> Eulerverfahren ist ein Verfahren erster Ordnung

Euler-Cromer ist auch ein Verfahren erster Ordnung

### Mittelpunkt-Algorithmus

x_{n+1} = x_n + (v_{n+1} + v_n) / 2 \Delta_t

(i) =>
x_{n+1} = x_n + v_n \Delta_t + 1/2 a_n \Delta_t^2 + \BigO{\Delta_t^3}
=> Zweite Ordnung bez"uglich des Ortes
    erste Ordnung bez"uglich der Geschwindigkeit

### Halbschritt-Algorithmus

v_{n+1/2} = v_{n-1/2} + a_n \Delta_t
x_{n+1} = x_n + v_{n+1/2} \Delta_t

Nachteile
Nicht selbststartend (v_{-1/2} existiert nicht)
=> erster Schritt muss z.B. "uber Euler-Algorithmus berechnet werden
v_{1/2} = v_0 + a_0 \Delta_t/2

### Euler-Richardson-Algorithmus

Idee: Kombination von Vollschritt und Halbschritt, so dass alle 
\BigO{\Delta_t^2}-Terme verschwinden
  Buch: Anhang 3a

a_n = F(x_n, v_n, t_n)/m
v_{n+1/2} = v_n + a_n \Delta_t/2
x_{n+1/2} = x_n + v_v \Delta_t/2
a_{n+1/2} = F(x_{n+1/2}, v_{n+1/2}, t_{n+1/2})/m

x_{n+1} = x_n + v_{n+1/2} \Delta_t + \BigO{\Delta_t^3}
v_{n+1} = v_n + a_{n+1/2} \Delta_t + \BigO{\Delta_t^3}

gesamtes Verfahren hat globalen Fehler zweiter Ordnung
==> Algorithmus zweiter Ordnung

Diese Vorgehensweise der Kombination verschiedener Verfahren zur Ausl"oschung
ungewollter Terme ist angeblich typisch.
Hier haben wir die Vollschritte sowie die Halbschritte berechnet, so dass wir
die Differenz zwischen n und n+1/2 nutzen k"onnen, um die Schrittweite zu steuern.
Somit k"onnen wir in diesem Verfahren eine adaptive Schrittweitenkontrolle
einf"uhren und somit Konvergenz und Rechenzeit verbessern.
Auf den ersten Blick machen wir doppelt so viele Berechnungen, jedoch geht
hier der Halbschritt nicht in die Konvergenz ein, was der eigentliche Vorteil
des Verfahrens ist.
Somit gewinnen wir sehr viel f"ur die gew"unschte Genauigkeit.

### Verlet-Algorithmus (Leap-Frog)

Idee: Umgekehrte Taylorentwicklung

x_{n-1} = x_n - v_n \Delta_t + 1/2 a_n \Delta_t^2 + \BigO(\Delta_t^3) (iii)
(iii) entspricht also (ii) rueckwaerts. Wir addieren beide:

x_{n+1} = 2x_n - x_{n-1} + a_n \Delta_t^2 + \BigO(\Delta_t^4)
Wir verlieren die Terme dritter Ordnung und behalten Terme vierter Ordnung.

Wir k"onnen auch beide subtrahieren:
v_n = (x_{n+1} - x_{n-1})/(2 \Delta_t) + \BigO{\Delta_t^3}

Somit erhalten wir ein Verfahren, das dritter Ordnung im Ort und zweiter
Ordnung in der Geschwindigkeit ist, welches ebenfalls nicht selbststartend
ist.

### Velocity-Verlet-Algorithmus

x_{n+1} = x_n + v_n \Delta_t + 1/2 a_n \Delta_t^2
v_{n+1} = v_n + (a_{n+1} + a_n)/2 * \Delta_t

Es tauchen nur ganzzahlige n auf, so dass der Algorithmus selbststartend ist.
Untersuchungen zeigen, dass die Energieerhaltung aus numerischen Gr"unden
besser funktioniert.

### Runge-Kutta-Algorithmus

(hier nicht im Detail erklaert)

RK N-ter Ordnung mit beinahe beliebigen ganzzahligen n
RK ist ein verallgemeinerter Euler-Richardson-Algorithmus

Hier kombiniert man N Schritte beliebiger L"ange per Addition und Subtraktion
der entsprechenden Gleichungen mit beliebigem Gewicht.
Gewichte in der Kombination, dass Terme mit \BigO{\Delta_t^i} verschwinden.

Euler-Richardson ist somit RK 2. Ordnung.

Standard: RK 4. Ordnung (siehe Buch)

Besonders: Der Halbschritt taucht doppelt auf, jedoch mit unterschiedlicher
Gewichtung (_"uberraschend_)

Nicht selbstverst"andlich: *Fehlerkontrolle*
Man rechnet mir Zeitschrittweite t und mit halber Zeitschrittweite und
vergleicht beide miteinander.
In RK erh"oht man oftmals die Ordnung und vergleicht Ergebnisse unterschiedlicher Ordnung (auch in OSP: RK45 in numerics-Package)

_RK45_
Fehlerkontrolle per Vergleich der Ergebnisse 4. und 5. Ordnung

### Adaptive Algorithmen

_ODEMultiStepSolver_ (ODE: Ordinary Differential Equation)
Dynamische Zeitschrittweite
Zielzeitschritt muss entsprechend "uberprueft werden, ist hier zum Glueck
schon implementiert.

### Richardson-Extrapolation

Taylorentwicklung in beide Zeitrichtungen auch in der Geschwindigkeit.
Kombination zu Verfahren vierter Ordnung, mit dem man h"ohere Ordnungen
rekursiv berechnen kann.

### Pr"adiktor-Korrektor-Verfahren

Idee: Eine erste L"osung wird vorausberechnet f"ur x_{n+1}, "uberpr"uft und
ggf. korrigiert.
Heutzutage der anspruchsvollste Standard-Algorithmus bei Simulationspaketen.

einfachste Darstellung eines PK-Verfahrens:

\tilde{x}_{n+1} = x_{n-1} + 2 v_n \Delta_t
\tilde{a}_{n+1} = F(\tilde{x}_{n+1}) / m
v_{n+1} = v_n + (a_n + \tilde{a}_{n+1})/2 \Delta_t
x_{n+1} = x_n + (v_{n+1} + v_n)/2 \Delta_t
NEUBERECHNUNG: \tilde{a}_{n+1}, v_{n+1} und x{n+1}, bis wir zufrieden sind
Erst danach: \tilde{x}_{n+2} usw.

Vorteil:
Zus"atzliche Konvergenz, so dass das Programm die Zahl der Iterationsschritte
an die notwendige Genauigkeit anpasst.

Oftmals wird dieses Verfahren bei anspruchsvollen Simulationen genutzt,
ansonsten RK4 oder RK45

Di 16. Okt 11:27:12 CEST 2012
